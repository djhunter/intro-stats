---
title: "Chapter 22: Inference for comparing many means"
output: 
  revealjs::revealjs_presentation:
    fig_width: 14 
    fig_height: 7
    self_contained: true
    theme: night
    highlight: zenburn
    css: slidesdjh.css
    center: false
    transition: slide
    reveal_options:
      controls: true
      progress: false
      width: 1080
      height: 540
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
options(width = 100)
library(tidyverse)
library(openintro)
library(kableExtra)
theme_set(theme_bw())
set.seed(1423)
```

# Overview

- 22.1 Case study: Batting
- 22.2 Randomization test for comparing many means
    - 22.2.1 Observed data
    - 22.2.2 Variability of the statistic
    - 22.2.3 Observed statistic vs. null statistic
- 22.3 Mathematical model for test for comparing many means
    - 22.3.1 Variability of the statistic
    - 22.3.2 Observed statistic vs. null statistics
    - 22.3.3 Reading an ANOVA table from software
    - 22.3.4 Conditions for an ANOVA analysis

# 22.1 Case study: Batting

## Research Question

Is a baseball player's on-base-percentage (OBP) associated with their defensive position?

## 22.2.1 Observed data

```{r mlb_players_18-data-prep, include=FALSE}
players <- mlb_players_18 %>%
  filter(
    AB >= 100, 
    !position %in% c("P", "DH")
  ) %>%
  mutate(
    position = case_when(
      position %in% c("LF", "CF", "RF")       ~ "OF",
      position %in% c("1B", "2B", "3B", "SS") ~ "IF",
      TRUE                                    ~ position
    ),
    position = fct_relevel(position, "OF", "IF", "C")
  )
```

```{r}
glimpse(players)
```
## Data summary

```{r}
players %>%
  group_by(position) %>%
  summarize(n = n(),
            Mean = mean(OBP),
            SD = sd(OBP))
```

## Side-by-side Box Plots

```{r, fig.height=4}
ggplot(players, aes(x = position, y = OBP)) +
  geom_boxplot()
```

## t-test: OF vs IF

```{r}
players %>%
  filter(position %in% c("OF", "IF")) %>%
  t.test(OBP ~ position, data = .)
```

## t-test: OF vs C

```{r}
players %>%
  filter(position %in% c("OF", "C")) %>%
  t.test(OBP ~ position, data = .)
```

## t-test: C vs IF

```{r}
players %>%
  filter(position %in% c("C", "IF")) %>%
  t.test(OBP ~ position, data = .)
```



## P-hacking

It is *not* good practice to take several groups and compare selected pairs.

>- Even if there is no association, 5% of t-tests will show a p-value less than 0.05. This is a consequence of random sampling.
>- Given enough groups to compare, you will eventually find a significant difference just by chance, even if no association is present.
>- This is the *Multiple Comparisons Problem*, aka, "data snooping", "data fishing", or "p-hacking"

## Omnibus Tests

**Best Practice:** 

- First: Apply an *omnibus test* to check for an overall association.
- If the omnibus test shows a significant association, it is OK to follow up with pairwise comparisons.

# 22.2 Randomization test for comparing many means

## Multiple mean hypotheses

- $H_0$: The mean outcome is the same across all groups.
    - $H_0: \mu_1 = \mu_2 = \cdots = \mu_k$
- $H_A$: At least one mean is different.

## Hypotheses (Batting)

- $H_0: \mu_{OF} = \mu_{IF} = \mu_{C}$ (The average OBP is the same for outfielders, infielders, and catchers).
- $H_A:$ The average OBP is not the same for all three groups.

## The F-Statistic

- We want a statistic that measures *all* the groups at once, and reflects the overall difference.
- The *F-statistic* is the fraction $\displaystyle{\frac{MSG}{MSE}}$
    - $MSG$ measures the *between-group* variability.
    - $MSE$ measures the *within-group* variability.
- The bigger the F-statistic, the more different the groups are, taking into account the variability within the groups.

## F-Statistic Formula (just FYI)

\[ 
\begin{align}
F &= \frac{\frac{1}{I-1} \sum_{i=1}^I n_i(\bar{x}_i - \bar{x})^2}{\frac{1}{N-I}\sum_{i=1}^I(n_i-1)s_i^2} \\[5pt]
  &= \frac{\text{between-group variability}}{\text{within-group variability}} \\[5pt]
  &= \frac{\text{variability due to explanatory variable}}{\text{unexplained variability}}
\end{align}
\]

## Group Activity

```{r, echo=FALSE, message=FALSE, fig.height=3.5}
m1 <- c(3,5,4)
m2 <- c(2, 6, 4)
noise <- c(0,0,0,-0.25,-0.25,-0.25,0.25,0.25,0.25,0.1,0.1,0.1)
group <- factor(c(1,1,1,2,2,2,3,3,3,4,4,4))
Plot <- c(rep("A", 12), rep("B", 12), rep("D", 12), rep("C",12))
bp <- data.frame(x = c(m1+noise, m2 + noise, m1 + 2*noise, m2+2*noise), group = rep(group, 4), Plot = Plot)
p <- ggplot(bp, aes(x = x, y = group, fill=group)) + geom_boxplot()
p + facet_wrap(vars(Plot)) + theme_bw() + guides(fill="none")
```

1. Which data sets (A,B,C,D) have the most *within-group* variability?
2. Which data sets have the most *between-group* variablilty?
3. Which data set has the largest F-statistic?

## 22.2.2 Variability of the statistic

Simulation using cards:

- 429 cards, with the OPB written on each
- Deal into 3 piles, 160 OFs, 205IFs, 64Cs
- Compute simulated F-statistic
    - These statistics make a *null-distribution*
- Compute P-value by finding the proportion of the null distribution that are as or more extreme than the observed F-statistic.

# Exercise and Brain Volume

- Researchers randomly assigned elderly adult volunteers into four activity groups: tai chi, walking, social interaction, and no intervention. 
- 120 participants started the study, and 13 dropped out along the way, so 107 completed the study.
- Except for the control group, each group met for about an hour three times a week for 40 weeks.
- Each participant had an MRI to determine brain size before the study began and again at its end. The researchers measured the percentage increase or decrease in brain size during that time. 

## Research question

- **Question:** Can different types of activities affect brain shrinkage in elders?
- **Categorical Explanatory:** Activity type: tai chi, walking, social interaction, no intervention
- **Quantitative Response:** Change in brain size (using MRI)

## Hypotheses

- Null: There is no association between the type of activity and the change in brain size.

\[ H_0: \mu_\text{TaiChi}=\mu_\text{walking}= \mu_\text{social}=\mu_\text{none} \]

- Alternative: There is an association between the type of activity and the change in brain size.

\[ H_a: \text{At least one of } \mu_\text{TaiChi},\mu_\text{walking}, \mu_\text{social},\mu_\text{none} \\ 
 \text{is different from the others}
\]

## Group Activity

Open the [Multiple Means applet](http://www.rossmanchance.com/applets/AnovaShuffle.htm?hideExtras=2), select the *Brain* data from the menu, and press *Use Data*.

1. Which activity group tended to have the largest increase in brain size percentage change? Which tended to have the smallest increase (i.e., largest decrease)?

2. In the *Select Statistic* dropdown, choose the F-statistic, and record it.

3. Use at least 1000 shuffles to create a null distribution for the F-statistic. Describe the shape of the null distribution (symmetric? skewed?).
    
4. Use the null distribution to **record** a *p-value* for our hypothesis test, along with a *sentence* interpreting this p-value in the context of this problem.

# 22.3 Mathematical model for test for comparing many means

The F statistic is a ratio of how the groups differ (MSG) as compared to how the observations within a group vary (MSE).

$$
F = \frac{MSG}{MSE}
$$

Conditions:

- independent observations, both within and across groups
- large samples and no extreme outliers

Then the F-statistic has an F-distribution with $df_1 = k - 1$ and $df_2 = n - k$. ($n$ observations in $k$ groups)

## F-statistic in R

```{r}
summary(aov(OBP ~ position, data = players))
1 - pf(5.077, df1 = 2, df2 = 426)
```

## 22.3.3 Reading an ANOVA table from software

```{r}
anova(aov(OBP ~ position, data = players))
```


## 22.3.4 Conditions for an ANOVA analysis

- Independence (e.g., groups are randomly assigned)
- Normality (Each group should have a symmetric distribution, or a large sample)
- Constant variance (the SD of each group should be approximately equal.)


# Comparing Diets

- Explanatory: Diet type (Atkins, Zone, LEARN, Ornish)
    + Categorical, four levels
- Response: Change in [Body Mass Index](https://www.nhlbi.nih.gov/health/educational/lose_wt/BMI/bmicalc.htm) (BMI)
    + Quantitative (negative means it went down)

## Preview

- $H_0: \mu_\text{Atkins}=\mu_\text{Zone}= \mu_\text{LEARN}=\mu_\text{Ornish}$
- $H_a:$ At least one of $\mu_\text{Atkins},\mu_\text{Zone}, \mu_\text{LEARN},\mu_\text{Ornish}$ is different from the others.

## Group Activity

1. Open [Multiple Means applet](http://www.rossmanchance.com/applets/AnovaShuffle.htm?hideExtras=2) and choose the *Diets* data. Select the Statistic pull-down menu (on the left) to select $F$-statistic. Record the observed value of the $F$-statistic, and compute a p-value using a **simulated null distribution.**

2. **Record** a sentence interpreting this p-value in the context of the study.

3. Check the box to *Overlay F distribution*. **Record** a theory-based p-value for the above test using the ANOVA $F$-test. Compare the two p-values you found: simulation-based using $F$, and theory-based using the ANOVA $F$-test.

## ANOVA in R

```{r}
diets <- read.table("http://www.isi-stats.com/isi/data/chap9/Diets.txt", header = TRUE)
diet.aov <- aov(BMI ~ Diet, data = diets)
anova(diet.aov)
```


## Follow-up

*If an ANOVA test finds evidence that there is at least one long-run or population mean that is different, there are many different follow-up tests designed to help pinpoint where difference(s) occur. One option is to look at all the pairwise theory-based confidence intervals for the difference in long-run or population means.*

4. Use the applet to compute the (six) follow-up confidence intervals. **Record** which differences are significant. Is one diet clearly better than the others?

## Alternative Follow-up in R

5. [Tukey's Honestly Significant Differences Test](https://en.wikipedia.org/wiki/Tukey%27s_range_test) is a more conservative follow-up procedure. **Record** which differences are significant according to the TukeyHSD test, and compare to #4.

```{r}
TukeyHSD(diet.aov)
```

# P-Hacking Demo

## Simulated Data

- 10 groups: (shirt color) 
- Response variable: IQ (randomly assigned)

```{r}
set.seed(2434)
shirt <- factor(c("red", "green", "blue", "orange", "gold", 
                  "yellow", "brown", "pink", "gray", "purple"))
shirtIQ <- data.frame(
  shirt = sample(shirt, 2000, replace=TRUE), 
  IQ = rnorm(2000, mean = 100, sd = 15))
```

## Side-by-side box plots

```{r, fig.height=3.8, fig.width=6, fig.align='center'}
shirtIQ %>%
  ggplot(aes(x = IQ, y = shirt)) +
  geom_boxplot()
```


## Omnibus ANOVA test

```{r}
shirt.aov <- aov(IQ ~ shirt, data = shirtIQ)
anova(shirt.aov)
```

**Question:** Based on this ANOVA test, what should you do next?

## What if we follow-up anyway?

```{r}
pairwise.t.test(shirtIQ$IQ, shirtIQ$shirt, p.adjust.method = "none")
```

**Question:** Are any of the pairwise t-tests significant at $\alpha = 0.05$?

## P-hacking: Choose the pair you found

```{r}
shirtIQ %>%
  filter(shirt %in% c("purple", "brown")) %>%
  t.test(IQ ~ shirt, data = .)
```

## Note: Tukey HSD doesn't make this error

```{r}
TukeyHSD(shirt.aov)
```

