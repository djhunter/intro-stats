---
title: "Chapter 25: Inference for multiple linear regression"
output: 
  revealjs::revealjs_presentation:
    fig_width: 14 
    fig_height: 7
    self_contained: true
    theme: night
    highlight: zenburn
    css: slidesdjh.css
    center: false
    transition: slide
    reveal_options:
      controls: true
      progress: false
      width: 1080
      height: 540
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
options(width = 100)
library(tidyverse)
library(openintro)
library(kableExtra)
theme_set(theme_bw())
set.seed(1423)
```

# Overview

- 25.1 Multiple regression output from software
- 25.2 Multicollinearity
    - Cereal: Creating Multiple Regression Models
- 25.3 Cross-validation for prediction error

# 25.1 Multiple regression output from software

## Data on loans

- Response variable: interest rate
- Explanatory variables: debt to income ratio, term of loan, number of credit checks

```{r include=FALSE}
loans <- loans_full_schema %>%
  mutate(
    credit_util = total_credit_utilized / total_credit_limit,
    bankruptcy = as.factor(if_else(public_record_bankrupt == 0, 0, 1)),
    verified_income = droplevels(verified_income)
  ) %>%
  rename(credit_checks = inquiries_last_12m) %>%
  select(interest_rate, verified_income, debt_to_income, credit_util, bankruptcy, term, credit_checks, issue_month)
```

```{r}
glimpse(loans)
```

## Regression model

$$
\begin{aligned}
\texttt{interest_rate} = \beta_0 &+ \beta_1\times \texttt{debt_to_income} \\
&+ \beta_2 \times \texttt{term}\\
&+ \beta_3 \times \texttt{credit_checks}\\
&+ \varepsilon
\end{aligned}
$$

The model coefficients $\beta_0, \beta_1, \beta_2, \beta_3$ are unknown parameters. The $\varepsilon$ models the residuals, which are assumed to be normally distributed. 

## Regression output 

```{r}
summary(lm(interest_rate ~ debt_to_income + term + credit_checks, data = loans))
```

## Regression Table and Equation

```{r echo=FALSE}
loan.model <- lm(interest_rate ~ debt_to_income + term + credit_checks, data = loans)
round(summary(loan.model)$coefficients, 2)
```

$$
\begin{aligned}
\widehat{\texttt{interest_rate}} = 4.31 &+ 0.041 \times \texttt{debt_to_income} \\
&+ 0.16 \times \texttt{term} \\
&+ 0.25 \times \texttt{credit_checks}
\end{aligned}
$$

## Interpreting the P-values

Each P-value corresponds to a different two-sided hypothesis test:

- $H_0: \beta_1 = 0$, given `term` and `credit_checks` are included in the model
- $H_0: \beta_2 = 0$, given `debt_to_income` and `credit_checks` are included in the model
- $H_0: \beta_3 = 0$, given `debt_to_income` and `term` are included in the model

We say that these hypotheses are *conditioned on* the other variables in the model 

- e.g., we have strong evidence that `term` is positively associated with interest rate, when the other variables are taken into account.

## Condtioning on other variables

In a multivariable model, the P-values for each coefficient measure the effect of the variable *assuming the other variables remain constant*.

```{r echo=FALSE}
loan.model <- lm(interest_rate ~ debt_to_income + term + credit_checks, data = loans)
round(summary(loan.model)$coefficients, 2)
```

- *For loans with a given term and borrowers with a given debt-to-income ratio,* we have very strong evidence that the number of credit checks has a positive association with interest rate.
- The number of credit checks has a significant effect, *even when term and debt-to-income ratio are taken into account.*
- *Controlling term and debt-to-income ratio,* the number of credit checks has a significant effect. 

# Penguins revisited

## Data Carpentry Script

```{r, eval = FALSE}
library(tidyverse)
library(palmerpenguins)

glimpse(penguins)
ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +
  geom_point()

bfit1 <- lm(bill_depth_mm ~ bill_length_mm, data = penguins)
summary(bfit1)

## This model reports a negative association, which doesn't make sense.
## Challenge: Make a new multiple regression model using species as an 
## additional response variable.
## 1. Write down the multiple regression equation.
## 2. For each coefficient whose effect is significant (p-val < 0.01),
## write a sentence interpreting the p-value.
```

# 25.2 Multicollinearity

## Example: Predict the value of a coin dish

- Response Variable: Total value of coins
- Explanatory Variables: total number of coins, total number of low coins (i.e., coins smaller than quarters)

## {data-background="https://openintro-ims.netlify.app/images/money.png" data-background-size="contain"}

```{r include=FALSE}
money <- tibble(
  number_of_coins = c(9, 10, 3, 5, 10, 37, 28, 9, 11, 4, 6, 17, 15, 7, 9, 1, 5, 9, 36, 30, 47, 13, 5, 7, 18, 16),
  number_of_low_coins = c(4, 8, 0, 4, 9, 34, 9, 3, 2, 2, 5, 12, 11, 4, 8, 0, 4, 9, 34, 9, 3, 2, 2, 5, 12, 11),
  total_amount = c(1.37, 1.01, 1.5, 0.56, 0.61, 3.06, 5.42, 1.75, 5.4, 0.56, 0.34, 2.33, 3.34, 1.3, 1.2, 1.7, 0.86, 0.61, 2.96, 5.52, 8.95, 5.2, 1.56, 0.74, 1.83, 3.74)
)
```

## {data-background="https://openintro-ims.netlify.app/25-inf-model-mlr_files/figure-html/coinfig-1.png" data-background-size="contain"}

## Predicting amount: single variable models

```{r}
summary(lm(total_amount ~ number_of_coins, data = money))$coefficients
```

```{r}
summary(lm(total_amount ~ number_of_low_coins, data = money))$coefficients
```

## Predicting amount: multivariable model

```{r}
summary(lm(total_amount ~ number_of_coins + number_of_low_coins, data = money))$coefficients
```

## Compare the three models

```{r echo=FALSE}
round(summary(lm(total_amount ~ number_of_coins, data = money))$coefficients, 2)
round(summary(lm(total_amount ~ number_of_low_coins, data = money))$coefficients, 2)
round(summary(lm(total_amount ~ number_of_coins + number_of_low_coins, data = money))$coefficients, 2)
```

## Group Activity

1. In the first model, is the number of coins associated to the total amount? Does this make sense?

2. In the second model, is the number of low coins associated to the total amount? Does this make sense?

3. In the third model, how are these associations different? Can you explain why they would be different?

## Multicollinearity

```{r}
cor(money)
```

*Multicollinearity* happens when the predictor variables are correlated within themselves.

- Hard to interpret the regression coefficients.
- Still OK for making predictions using the regression equation.

## Loan Data Correlations

```{r}
round(cor(loans[, c(1,3,6,7)], use = "na.or.complete"), 3)
```

## Conditioning on other variables

```{r echo=FALSE}
round(summary(lm(total_amount ~ number_of_coins, data = money))$coefficients, 2)
round(summary(lm(total_amount ~ number_of_low_coins, data = money))$coefficients, 2)
round(summary(lm(total_amount ~ number_of_coins + number_of_low_coins, data = money))$coefficients, 2)
```

- If we *hold the number of coins constant*, then the number of low coins has a negative association.


# Cereal Ratings: Other Variables

- Last time, we found that sugar content was a good predictor of cereal rating. 
- In addition to sugar content, the researchers recorded several other variables for each box of cereal. Using these variables, can we predict how a cereal would be rated?

## Cereal Data

```{r echo=FALSE}
cereal <- read.table("http://math.westmont.edu/data/cereal.csv", header=TRUE, sep=";")
glimpse(cereal)
```

## Group Activity

```{r}
round(cor(cereal[,c(4,5,6,7,8,9,10,12,16)]),2)
```

1. Which two measurements have the strongest positive association with `rating`? Which two have the strongest negative association? 
2. Which two predictor variables are most strongly correlated?

## Explore five variables of the data

```{r}
glimpse(cereal[,c(4,5,8,10,16)])
```

## Regression: `rating ~ sugars`

```{r}
summary(lm(rating ~ sugars, data = cereal))
```

## Regression: `rating ~ fiber`

```{r}
summary(lm(rating ~ fiber, data = cereal))
```

## Multiple Regression

- There is more than one choice for an explanatory variable.
- Different choices have different $r^2$'s. 
- Multiple regression gives us a way to combine them. It also gives us a combined coefficient of determination, $R^2$.
- All we have to do is change the formula.

## `rating ~ sugars + fiber`

```{r}
summary(lm(rating ~ sugars + fiber, data = cereal))
```

## Multiple regression prediction equation

- We can write one prediction equation.
- The relationship explains 82% of the variability in rating: $R^2 = 0.8165$

$$\widehat{\text{rating}} = 51.6 - 2.2\times\text{sugars} + 2.9\times\text{fiber}$$

## More explanatory variables

```{r}
summary(lm(rating ~ sugars + fiber + calories + protein, data = cereal))
```

## Group Exercise

1. **Record** a prediction equation for rating based on the explanatory variables sugars, fiber, calories, and protein.

2. **Record** the percentage of variability in rating that is determined by this relationship.

3. A new cereal, *Sugar Frosted Chocolate Bombs*, has 140 calories per serving, 2 grams of protein, 16 grams of sugars, and 1 gram of fiber. **Record** a predicted rating for this cereal.

## {data-background="http://math.westmont.edu/img/sugar.jpg" data-background-size="contain"}

## Two types of p-values

- p-values for each coefficient: $H_0: \beta_1 = 0, \, H_0: \beta_2 = 0, \, \ldots, H_0: \beta_k = 0$
- p-values for an overall association: $H_0: \beta_1 = \beta_2 = \cdots = \beta_k = 0$

Check overall significance *before* looking at individual slopes. 

## Validity Conditions

- Validity conditions for simple linear regression for each variable:
    + linear relationship
    + independent observations
    + symmetry across regression line
    + equal variation along regression line

## Overfitting

- In multiple regression, adding more explanatory variables does not always make the model better.
- Too many variables can give a misleadingly high $R^2$. It can also amplify noise in the data and make predictions worse (overfitting).
- *Rule of thumb:* if you add an explanatory variable and the *Adjusted R-squared* goes down, you have too many variables.

## Five-variable model

Adjusted $R^2 \approx 0.8923$.

```{r}
summary(lm(rating ~ sugars + fiber + calories + protein + fat, data = cereal))
```

## Six-variable model

Adjusted $R^2 \approx 0.8908$.

```{r}
summary(lm(rating ~ sugars + fiber + calories + protein + fat + carbo, data = cereal))
```

## Sometimes less is more

- When we added `carbo` to the model, the Adjusted R-squared went down.
- This means the additional explanatory variable didn't improve the model.
- Having too many explanatory variables can throw off predictions.

# 25.3 Cross-validation for prediction error

## Comparing Models

- **Goal:** We want to assess how well our models will predict *future observations* that are not in our data set.
- In **Cross Validation**, you fit the model on part of the data set, and then you test the residuals on the rest of the data set.
    - The "held out" data plays the role of future observations.

## Cross Validation in R

```{r, message=FALSE}
library(caret)
set.seed(1234)
tc <- trainControl(method = "cv", number = 5)
train(rating ~ sugars + fiber, data = cereal, method = "lm", trControl = tc)
```

---


```{r}
train(rating ~ sugars + fiber + calories + protein + fat, data = cereal, method = "lm", trControl = tc)
```

---

```{r}
train(rating ~ sugars + fiber + calories + protein + fat + carbo, data = cereal, method = "lm", trControl = tc)
```
